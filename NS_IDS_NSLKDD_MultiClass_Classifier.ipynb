{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NSL-KDD MultiClass Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NSL-KDD dataset has data in the following categories, Normal , DoS, Probe, R2L, U2R.KNN, Support Vector Machine(SVM), Decision Tree , Random Forest , Logistic Regression and XGboost learning algorithms are trained separately and their performances are analysed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing essential libraries\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "import cv2\n",
    "import random \n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the training and test dataset \n",
    "pickle_in = open(\"X_train_NSL_MC.pickle\",\"rb\")\n",
    "X_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y_train_NSL_MC.pickle\",\"rb\")\n",
    "y_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"X_test_NSL_MC.pickle\",\"rb\")\n",
    "X_test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y_test_NSL_MC.pickle\",\"rb\")\n",
    "y_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train  (125973, 41)\n",
      "y_train  (125973, 1)\n",
      "X_test  (22544, 41)\n",
      "y_test  (22544, 1)\n"
     ]
    }
   ],
   "source": [
    "#shape of the training and test datasets \n",
    "print('X_train ',X_train.shape)\n",
    "print('y_train ',y_train.shape)\n",
    "print('X_test ',X_test.shape)\n",
    "print('y_test ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 67343, 1: 45927, 2: 11656, 3: 995, 4: 52}\n"
     ]
    }
   ],
   "source": [
    "digit_train, counts_train = np.unique(y_train, return_counts = True)\n",
    "\n",
    "distribution_train = dict(zip(digit_train, counts_train))\n",
    "print(distribution_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN  Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train,y_train) #training the model \n",
    "knn_prediction = knn_model.predict(X_test) #Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is:  0.9953323331190017\n",
      "Test score is:  0.7274662881476224\n"
     ]
    }
   ],
   "source": [
    "#Evaluating Performance\n",
    "print(\"Train score is: \", knn_model.score(X_train, y_train))\n",
    "print(\"Test score is: \",knn_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[9435 1589  648 2583  100]\n",
      " [ 131 5548  370    2   59]\n",
      " [ 137  321 1403  163   33]\n",
      " [   8    0    0    6    0]\n",
      " [   0    0    0    0    8]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.66      0.78     14355\n",
      "           1       0.74      0.91      0.82      6110\n",
      "           2       0.58      0.68      0.63      2057\n",
      "           3       0.00      0.43      0.00        14\n",
      "           4       0.04      1.00      0.08         8\n",
      "\n",
      "    accuracy                           0.73     22544\n",
      "   macro avg       0.47      0.74      0.46     22544\n",
      "weighted avg       0.87      0.73      0.78     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(knn_prediction, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(knn_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing SVC Classifier\n",
    "from sklearn.svm import SVC\n",
    "svc_model=SVC(random_state=0)\n",
    "svc_model.fit(X_train, y_train) #training the model \n",
    "svc_prediction = svc_model.predict(X_test) #Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is:  0.5351146674287347\n",
      "Test score is:  0.43066891412349184\n"
     ]
    }
   ],
   "source": [
    "#Evaluating Performance\n",
    "print(\"Train score is: \", svc_model.score(X_train, y_train))\n",
    "print(\"Test score is: \",svc_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[9709 7458 2421 2752  200]\n",
      " [   0    0    0    0    0]\n",
      " [   0    0    0    2    0]\n",
      " [   2    0    0    0    0]\n",
      " [   0    0    0    0    0]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.43      0.60     22540\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.43     22544\n",
      "   macro avg       0.20      0.09      0.12     22544\n",
      "weighted avg       1.00      0.43      0.60     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(svc_prediction, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(svc_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model=LogisticRegression(random_state=0)\n",
    "lr_model.fit(X_train, y_train.ravel()) #training the model \n",
    "lr_prediction = lr_model.predict(X_test) #Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is:  0.8427202654537084\n",
      "Test score is:  0.6282381121362669\n"
     ]
    }
   ],
   "source": [
    "#Evaluating Performance\n",
    "print(\"Train score is: \", lr_model.score(X_train, y_train))\n",
    "print(\"Test score is: \",lr_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[9379 1717 1142 2118   81]\n",
      " [ 277 4784 1279   17   92]\n",
      " [  55  957    0  619   27]\n",
      " [   0    0    0    0    0]\n",
      " [   0    0    0    0    0]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.65      0.78     14437\n",
      "           1       0.64      0.74      0.69      6449\n",
      "           2       0.00      0.00      0.00      1658\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.63     22544\n",
      "   macro avg       0.32      0.28      0.29     22544\n",
      "weighted avg       0.80      0.63      0.69     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(lr_prediction, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(lr_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_model = DecisionTreeClassifier(max_depth=3)                   # decision tree\n",
    "dt_model.fit(X_train,y_train) #training the model \n",
    "dt_prediction = dt_model.predict(X_test) #Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is:  0.9524421899930937\n",
      "Test score is:  0.7233410220014195\n"
     ]
    }
   ],
   "source": [
    "#Evaluating Performance\n",
    "print(\"Train score is: \", dt_model.score(X_train, y_train))\n",
    "print(\"Test score is: \",dt_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[9477 1925  857 2525  163]\n",
      " [  57 5442  176    5   32]\n",
      " [ 177   91 1388  224    5]\n",
      " [   0    0    0    0    0]\n",
      " [   0    0    0    0    0]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.63      0.77     14947\n",
      "           1       0.73      0.95      0.83      5712\n",
      "           2       0.57      0.74      0.64      1885\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.72     22544\n",
      "   macro avg       0.46      0.46      0.45     22544\n",
      "weighted avg       0.88      0.72      0.77     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(dt_prediction, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(dt_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Random Forest\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_train, y_train.ravel()) #training the model \n",
    "rfc_prediction = rfc_model.predict(X_test) #Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is:  0.999944432537131\n",
      "Test score is:  0.7531937544357701\n"
     ]
    }
   ],
   "source": [
    "#Evaluating Performance\n",
    "print(\"Train score is: \", rfc_model.score(X_train, y_train))\n",
    "print(\"Test score is: \",rfc_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[9456 1394  819 2694  193]\n",
      " [  65 6024  162    0    0]\n",
      " [ 190   40 1440    2    1]\n",
      " [   0    0    0   57    3]\n",
      " [   0    0    0    1    3]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.65      0.78     14556\n",
      "           1       0.81      0.96      0.88      6251\n",
      "           2       0.59      0.86      0.70      1673\n",
      "           3       0.02      0.95      0.04        60\n",
      "           4       0.01      0.75      0.03         4\n",
      "\n",
      "    accuracy                           0.75     22544\n",
      "   macro avg       0.48      0.83      0.49     22544\n",
      "weighted avg       0.90      0.75      0.80     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(rfc_prediction, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(rfc_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing XGBoost \n",
    "from xgboost import XGBClassifier\n",
    "xgb_model =XGBClassifier(max_depth=9,\n",
    "                          subsample=0.9,\n",
    "                          objective='multi:softmax',\n",
    "                          num_class = 3,\n",
    "                          min_child_weight=2,\n",
    "                          colsample_bytree=0.7,\n",
    "                          n_estimators=1000,\n",
    "                          learning_rate=0.08,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "xgb_model.fit(X_train,y_train,verbose=True) #training the model \n",
    "xgb_prediction = xgb_model.predict(X_test) #Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is:  0.999944432537131\n",
      "Test score is:  0.7668115684882896\n"
     ]
    }
   ],
   "source": [
    "#Evaluating Performance\n",
    "print(\"Train score is: \", xgb_model.score(X_train, y_train))\n",
    "print(\"Test score is: \",xgb_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[9444 1199  751 2583  189]\n",
      " [  67 6162  163    0    0]\n",
      " [ 198   97 1507    1    2]\n",
      " [   1    0    0  169    4]\n",
      " [   1    0    0    1    5]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.67      0.79     14166\n",
      "           1       0.83      0.96      0.89      6392\n",
      "           2       0.62      0.83      0.71      1805\n",
      "           3       0.06      0.97      0.12       174\n",
      "           4       0.03      0.71      0.05         7\n",
      "\n",
      "    accuracy                           0.77     22544\n",
      "   macro avg       0.50      0.83      0.51     22544\n",
      "weighted avg       0.90      0.77      0.81     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(xgb_prediction, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(xgb_prediction, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
