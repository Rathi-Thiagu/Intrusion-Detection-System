{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NSL-KDD Binary Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NSL-KDD dataset is converted into two categories with '0' representing Normal Traffic and '1' representing the Attack. KNN, Support Vector Machine(SVM), Decision Tree , Random Forest , Logistic Regression and XGboost learning algorithms are trained separately and their performances are analysed separately.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing essential libraries\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "import cv2\n",
    "import random \n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the datasets from the pickle file \n",
    "pickle_in = open(\"X_train_NSL_B.pickle\",\"rb\")\n",
    "X_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y_train_NSL_B.pickle\",\"rb\")\n",
    "y_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"X_test_NSL_B.pickle\",\"rb\")\n",
    "X_test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y_test_NSL_B.pickle\",\"rb\")\n",
    "y_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train  (125973, 41)\n",
      "y_train  (125973, 1)\n",
      "X_test  (22544, 41)\n",
      "y_test  (22544, 1)\n"
     ]
    }
   ],
   "source": [
    "#shape of the training and test datasets \n",
    "print('X_train ',X_train.shape)\n",
    "print('y_train ',y_train.shape)\n",
    "print('X_test ',X_test.shape)\n",
    "print('y_test ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 67343, 1: 58630}\n"
     ]
    }
   ],
   "source": [
    "digit_train, counts_train = np.unique(y_train, return_counts = True)\n",
    "\n",
    "distribution_train = dict(zip(digit_train, counts_train))\n",
    "print(distribution_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train,y_train)    #training the model \n",
    "knn_prediction = knn_model.predict(X_test)  #Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is:  0.9971343065577545\n",
      "Test score is:  0.7699609652235628\n"
     ]
    }
   ],
   "source": [
    "#Evaluating performance\n",
    "print(\"Train score is: \", knn_model.score(X_train, y_train))\n",
    "print(\"Test score is: \",knn_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[9427 4902]\n",
      " [ 284 7931]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.66      0.78     14329\n",
      "           1       0.62      0.97      0.75      8215\n",
      "\n",
      "    accuracy                           0.77     22544\n",
      "   macro avg       0.79      0.81      0.77     22544\n",
      "weighted avg       0.84      0.77      0.77     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(knn_prediction, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(knn_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing SVC Classifier\n",
    "from sklearn.svm import SVC\n",
    "svc_model=SVC(random_state=0)\n",
    "svc_model.fit(X_train, y_train)  #training the model \n",
    "svc_prediction = svc_model.predict(X_test) #Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is:  0.535122605637716\n",
      "Test score is:  0.43080198722498225\n"
     ]
    }
   ],
   "source": [
    "#Evaluating performance\n",
    "print(\"Train score is: \", svc_model.score(X_train, y_train))\n",
    "print(\"Test score is: \",svc_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 9710 12831]\n",
      " [    1     2]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.43      0.60     22541\n",
      "           1       0.00      0.67      0.00         3\n",
      "\n",
      "    accuracy                           0.43     22544\n",
      "   macro avg       0.50      0.55      0.30     22544\n",
      "weighted avg       1.00      0.43      0.60     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(svc_prediction, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(svc_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "lr_model=LogisticRegression(random_state=0)\n",
    "lr_model.fit(X_train, y_train.ravel())  #training the model\n",
    "lr_prediction = lr_model.predict(X_test) #Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is:  0.8765926031768713\n",
      "Test score is:  0.7036462029808375\n"
     ]
    }
   ],
   "source": [
    "#Evaluating performance\n",
    "print(\"Train score is: \", lr_model.score(X_train, y_train))\n",
    "print(\"Test score is: \",lr_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[8746 5716]\n",
      " [ 965 7117]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.60      0.72     14462\n",
      "           1       0.55      0.88      0.68      8082\n",
      "\n",
      "    accuracy                           0.70     22544\n",
      "   macro avg       0.73      0.74      0.70     22544\n",
      "weighted avg       0.78      0.70      0.71     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(lr_prediction, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(lr_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_model = DecisionTreeClassifier(max_depth=3)                   # decision tree\n",
    "dt_model.fit(X_train,y_train)       #training the model \n",
    "dt_prediction = dt_model.predict(X_test) #Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is:  0.9585387344907242\n",
      "Test score is:  0.7876153300212917\n"
     ]
    }
   ],
   "source": [
    "#Evaluating performance\n",
    "print(\"Train score is: \", dt_model.score(X_train, y_train))\n",
    "print(\"Test score is: \",dt_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[9388 4465]\n",
      " [ 323 8368]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.68      0.80     13853\n",
      "           1       0.65      0.96      0.78      8691\n",
      "\n",
      "    accuracy                           0.79     22544\n",
      "   macro avg       0.81      0.82      0.79     22544\n",
      "weighted avg       0.85      0.79      0.79     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(dt_prediction, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(dt_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Random Forest\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_train, y_train.ravel())  #training the model \n",
    "rfc_prediction = rfc_model.predict(X_test) #Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is:  0.999944432537131\n",
      "Test score is:  0.7774574166075231\n"
     ]
    }
   ],
   "source": [
    "#Evaluating performance\n",
    "print(\"Train score is: \", rfc_model.score(X_train, y_train))\n",
    "print(\"Test score is: \",rfc_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[9449 4755]\n",
      " [ 262 8078]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.67      0.79     14204\n",
      "           1       0.63      0.97      0.76      8340\n",
      "\n",
      "    accuracy                           0.78     22544\n",
      "   macro avg       0.80      0.82      0.78     22544\n",
      "weighted avg       0.85      0.78      0.78     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(rfc_prediction, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(rfc_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:19:57] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Importing XGBoost \n",
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(silent=0,\n",
    "                     scale_pos_weight=1,\n",
    "                     learning_rate=0.01,\n",
    "                     colsample_bytree=0.6,\n",
    "                     subsample=0.8,\n",
    "                     objective='binary:logistic',\n",
    "                     n_estimators=100,\n",
    "                     reg_alpha=0.3,\n",
    "                     max_depth=3,\n",
    "                     gamma=1)\n",
    "\n",
    "xgb_model.fit(X_train,y_train,verbose=True)  #training the model \n",
    "xgb_prediction = xgb_model.predict(X_test) #Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is:  0.9798845784414121\n",
      "Test score is:  0.7590046132008517\n"
     ]
    }
   ],
   "source": [
    "#Evaluating performance\n",
    "print(\"Train score is: \", xgb_model.score(X_train, y_train))\n",
    "print(\"Test score is: \",xgb_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[9445 5167]\n",
      " [ 266 7666]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.65      0.78     14612\n",
      "           1       0.60      0.97      0.74      7932\n",
      "\n",
      "    accuracy                           0.76     22544\n",
      "   macro avg       0.78      0.81      0.76     22544\n",
      "weighted avg       0.84      0.76      0.76     22544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(xgb_prediction, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(xgb_prediction, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
